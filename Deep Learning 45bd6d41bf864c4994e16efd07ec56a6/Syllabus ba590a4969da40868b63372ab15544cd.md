# Syllabus

### **DS5B-605: Deep LearningÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Credits: 3 (2-0-2)**

**OBJECTIVE:**

This course aims to presentÂ the **mathematical, statistical and computational** challenges of building stable representations for **high-dimensional data**, such as **images, text and data**.Â We will delve into selected topics of Deep Learning, discussing recent models from both supervised and unsupervised learning. Special emphasis will be on c**onvolutional architectures, invariance learning, unsupervised learning and non-convex optimization.**

<aside>
ðŸ’¡ There will be 2 tests and assignments. Assignment do yourself, good assignment good marks. The more effort you make the better you will score. Ask doubts and stay active

</aside>

**COURSE DESCRIPTION:**

**Unit-I (This unit will be time taking around 10-14 hrs, 2 practical ANN)**

- Introduction of Deep learning
- Neural Network
    - Feed Forward Neural Network,
    - Back Forward Neural Network
- The Backpropagation algorithm
- Activation Function
    - Threshold
    - Sigmoid
    - Rectifier(ReLU)
    - Hyperbolic Tangent (tanh)
    - Gradient Descent
    - Stochastic Gradient Descent
    - Cost Function
    - Global minima and Local minima.

**Unit-II (only 2-2 lectures, dependent on first unit, only one practical of CNN maybe)**

- Convolutional Neural Networks
- Introduction of Keras
- Convolutional Neural Networks vs Human Brain
- Convolution Operation
- ReLU Layer
- Max Pooling Layer
- Data Flattening
- Fully Connected Layer
- Softmax & Cross-Entropy
- Data Augmentation
- Create image Dataset using Data Augmentation and Create CNN Model with Keras.

**Unit-III:** 

- Recurrent Neural Networks
- Uses of Recurrent Neural Networks
- Its application
- Issues with Feed Forward Network in RNN
- Backpropagation and Loss calculation
- Vanishing Gradient
- LSTM: Memory cell
- Forget gate
- Input gate
- Output gate
- Implement of RNN model in NLP with Keras.

**Unit-IV (This will be time taking and also has Practical part, only one practical of RNN maybe)**

- Boltzmann Machine
    - Energy-Based Models (EBM)
    - Restricted Boltzmann Machine
    - Contrastive Divergence
    - Deep Belief Networks
    - Deep Boltzmann Machines
- Self-Organizing Maps
    - Introduction Self-Organizing Maps
    - K-Means Clustering
    - Auto Encoders
    - Applications of Dynamic Memory Networks in NLP
    - Recent Research in NLP using Deep Learning
        - Fake news classifier
        - Similar question detection
        - Dialogue topic tracking.

**TEXTBOOKS**

Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "Deep learning."Â  MIT Press, (2015).

Bengio, Yoshua. "Learning deep architectures for AI." Foundations and trends in Machine Learning 2.1 (2009): 1127.

Hochreiter, Sepp, and Jargen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 17351780.

**Course Outcomes:**

After completing the study of the course the students are expected to:

- understand complexity of Deep Learning algorithms and their limitations;
- understand modern notions in data analysis oriented computing;
- be capable of confidently applying common Deep Learning algorithms in practice and implementing their own;
- be capable of performing distributed computations;
- be capable of performing experiments in Deep Learning using real-world data.